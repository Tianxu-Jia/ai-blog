{
  "hash": "c8c5d9b8aad93306fa005ab1362e808a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Utilising Ollama for Local LLM on Langchain\"\nauthor: \"Tianxu Jia\"\ntoc: true\nnumber-sections: true\ndate: today\ncategories: [ollama, langchain]\n---\n\n![](ollama-langchain.jpg)\n\nOpenAI debugging is expensive, while using free open-source LLMs during development is cost-effective. This allows delaying the decision between commercial or open-source for deployment.\n\nOllama makes open-source LLMs shine on Lanchain! This memo outlines the steps for using them seamlessly.\n\n## Setup\n- Download and Install Ollama <br>\n        curl https://ollama.ai/install.sh | sh\n\n- Pull the LLM model if needed: <br>\n        ollama pull llama2:7b-chat\n\n- Run the server: <br>\n        ollama serve<br>\n\n    Sometimes, the server doesn't need to be run because it has already begun after being installed correctly. \n\n## Usage for Chat<br>\n\n::: {#771586d2 .cell execution_count=1}\n``` {.python .cell-code}\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.chat_models import ChatOllama\n\nchat_model = ChatOllama(\n    model=\"llama2\",\n)\n\nfrom langchain.schema import HumanMessage\n\nmessages = [HumanMessage(content=\"Tell me about the history of AI\")]\naa = chat_model(messages)\n\nprint(aa.content)\n```\n:::\n\n\nIn some code, we can use `ChatOllama` replace `langchain.chat_models.ChatOpenAI`, for example in [here](https://docs.chainlit.io/integrations/langchain)\n\n## Ollama complete\n\n::: {#bfbe3bb7 .cell execution_count=2}\n``` {.python .cell-code}\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\n\naa = llm(\"Tell me about the history of AI\")\n\nprint(aa)\n```\n:::\n\n\n## Use Ollama as Embedding models\n\n::: {#68c3e425 .cell execution_count=3}\n``` {.python .cell-code}\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nembeddings = OllamaEmbeddings()\n\ntext = \"This is a test document.\"\n\nquery_result = embeddings.embed_query(text)\nprint(len(query_result))\nprint(query_result[:5])\n```\n:::\n\n\nembedding multiple documents:\n\n::: {#51834cc3 .cell execution_count=4}\n``` {.python .cell-code}\nfrom langchain_community.embeddings import OllamaEmbeddings\nimport numpy as np\n\nembeddings = OllamaEmbeddings()\n\ntext1 = \"this is a test document\"\ntest2 = \"How are you, Tom\"\ndoc_result = embeddings.embed_documents([text1, test2])\nprint(np.array(doc_result).ndim)\nprint(doc_result[0][:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n[0.604047954082489, -1.6594250202178955, -0.7071969509124756, 0.1293690800666809, -1.9412391185760498]\n```\n:::\n:::\n\n\n## OllamaFunctions\n\nWhile the current range of OllamaFunctions may be limited, Langchain offers an example for JSON-formatted output, demonstrating its potential for specific use cases.\n\n::: {#5a9d0242 .cell execution_count=5}\n``` {.python .cell-code}\nfrom langchain.chains import create_extraction_chain\n\n# Schema\nschema = {\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"height\": {\"type\": \"integer\"},\n        \"hair_color\": {\"type\": \"string\"},\n    },\n    \"required\": [\"name\", \"height\"],\n}\n\n# Input\ninput = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n\n# Run chain\nllm = OllamaFunctions(model=\"mistral\", temperature=0)\nchain = create_extraction_chain(schema, llm)\nchain.run(input)\n```\n:::\n\n\nThe output of the above code:\n\n```\n[{'name':       'Alex', \n  'height':      5, \n  'hair_color': 'blonde'},\n\n {'name':        'Claudia', \n  'height':       6, \n  'hair_color':  'brunette'}]\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}